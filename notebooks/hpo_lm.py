# We will be training an adapter on the language model `[CLS]` token embedding. 

 
#%pip install pandas torch torch-geometric matplotlib scikit-learn numpy


# We'll define our hyperparameters up top


import torch
import random
import numpy as np
torch.manual_seed(0)
random.seed(0)
np.random.seed(0)

BATCH_SIZE = 128
NORMALIZE_TARGETS = False

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cuda:0"


# To start off, we need to load the dataset. 


#More stupid python stuff
import sys
import os
root_dir = os.path.abspath(os.path.join(os.getcwd(), "..")) 
sys.path.append(root_dir)

from src.data.make_clsdataset import QM9CLS
from src.data.targets_dataset import TargetsDataset
import pandas as pd

data_dir = os.path.join(os.path.dirname(os.getcwd()), "data")

molecule_cls = QM9CLS("../data/etc/")
targets = TargetsDataset("../data/custom_qm9/raw", "../data/etc", normalize=True)


import torch

print(f"[CLS] Embedding Size: {molecule_cls[0].size()}")
print(f"First Molecule Target Tensor: {targets[0]}")
print(f"[CLS] Dataset Size: {torch.stack(molecule_cls.embeddings).size()}")
print(f"Target Dataset Size: {targets.targets.size()}")


# So now we have inputs `molecule_cls` and targets for regression `targets`. Note that we currently have `targets` as normalized between $-1$ and $1$. This could be changed later if neccessary. 


from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset


dataset = TensorDataset(torch.stack(molecule_cls.embeddings), targets.targets)
# train_size = int(len(dataset) * 0.8) # Use 80% of the dataset as a training dataset and leave the remaining 20% for testing
# valid_size = int(len(dataset) * 0.1)
split_indices_path = "../data/etc/split_idxs.pt"


assert(os.path.exists(split_indices_path)) # Split indices are generated by HydraGNN, so if they don't exist it should fail

train_indices, valid_indices, test_indices = torch.load(split_indices_path)


target_values = targets.targets[:,3] # Dipole moment $\mu$

# Normalize the target values

if NORMALIZE_TARGETS: 
    normalized_targets = (target_values - target_values.min()) / (target_values.max() - target_values.min())
else:
    normalized_targets = target_values
print(normalized_targets)

dataset = TensorDataset(torch.stack(molecule_cls.embeddings), normalized_targets)

#Create train and test datasets
train_dataset = Subset(dataset, train_indices)
valid_dataset = Subset(dataset, valid_indices)
test_dataset = Subset(dataset, test_indices)

print(len(train_dataset), len(valid_dataset), len(test_dataset))

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)

# print(next(iter(test_dataloader)))


os.environ["CUDA_VISIBLE_DEVICES"] = "0"


# Now, we need to train. We will use a neural network with 4 hidden layers with LeakyReLU activations and no output activation. We will use the AdamW optimizer and the MSE Loss


from tqdm import tqdm 
from math import sqrt
from torch import nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.nn.functional import mse_loss, l1_loss
import optuna
from optuna.study import MaxTrialsCallback
from optuna.trial import TrialState


num_trials = 100
num_jobs = 8
num_epochs = 30
def run(trial, return_model=False):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    model = nn.Sequential(
        nn.Linear(768, 500),
        nn.BatchNorm1d(500),
        nn.LeakyReLU(),
        nn.Dropout(p=dropout),  # Add dropout for regularization
        nn.Linear(500, 500),
        nn.BatchNorm1d(500),
        nn.LeakyReLU(),
        nn.Dropout(p=dropout),
        nn.Linear(500, 500),
        nn.BatchNorm1d(500),
        nn.LeakyReLU(),
        nn.Dropout(p=dropout),
        nn.Linear(500, 1),
        # nn.ReLU()
    ).to(device)

    optimizer = AdamW(model.parameters(), lr=trial.suggest_float("lr", 1e-6, 1e-1))
    loss_function = l1_loss #nn.HuberLoss(delta=0.3) #nn.L1Loss() #nn.SmoothL1Loss() #torch.nn.MSELoss()
    MAE_Loss = l1_loss

    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=trial.suggest_float("factor", 0.1, 0.9), patience=trial.suggest_int("patience", 3, 10), verbose=True) 

    best_valid_loss = float('inf')
    for i, epoch in enumerate(range(num_epochs)):
        if True:
            print(f"----Epoch {i+1}----")
        # Training phase
        model.train()  # Set model to training mode
        train_loss = 0.0

        for batch_idx, (inputs, _targets) in enumerate(tqdm(train_dataloader)):
            inputs, _targets = inputs.to(device), _targets.to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)

            # Calculate loss
            loss = loss_function(outputs.squeeze(1), _targets)

            # Backward pass and optimization
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
            optimizer.step()

            train_loss += loss.item() * inputs.size(0) # Accumulate batch loss
        train_loss /= len(train_dataloader.dataset) 

        model.eval()  # Set model to eval mode
        with torch.no_grad():
            valid_loss = 0.0
            valid_loss_mae = 0.0
            for batch_idx, (inputs, _targets) in enumerate(tqdm(valid_dataloader)): 
                inputs, _targets = inputs.to(device), _targets.to(device)
                outputs = model(inputs)

                # print(outputs[0], outputs[1])
                # Calculate loss
                loss = loss_function(outputs.squeeze(1), _targets)
                loss_mae = MAE_Loss(outputs.squeeze(1), _targets)

                valid_loss += loss.item() * inputs.size(0)
                valid_loss_mae += loss_mae.item() * inputs.size(0) 

            valid_loss /= len(test_dataloader.dataset)
            valid_loss_mae /= len(test_dataloader.dataset)
        
        if True:
            with torch.no_grad():
                model.eval()  # Set model to eval mode
                test_loss = 0.0
                test_loss_mae = 0.0
                for batch_idx, (inputs, _targets) in enumerate(tqdm(test_dataloader)): 
                    inputs, _targets = inputs.to(device), _targets.to(device)
                    outputs = model(inputs)
                    # print(outputs[0], outputs[1])
                    # Calculate loss
                    loss = loss_function(outputs.squeeze(1), _targets)
                    loss_mae = MAE_Loss(outputs.squeeze(1), _targets)
                    test_loss += loss.item() * inputs.size(0)
                    test_loss_mae += loss_mae.item() * inputs.size(0) 

                test_loss /= len(test_dataloader.dataset)
                test_loss_mae /= len(test_dataloader.dataset)

        trial.report(valid_loss, epoch)

        if trial.should_prune():
            raise optuna.TrialPruned()

        #Update LR based on test_loss
        scheduler.step(valid_loss)

        if True:
            print(f"Epoch {epoch+1} - Learning rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}") 
            print(f'Train Loss (MSE): {train_loss:.6f}, Validation Loss (MSE): {valid_loss:.6f}, Test Loss (MSE): {test_loss:.6f}')
            print(f'Validation RMSE: {sqrt(valid_loss):.6f}, Test RMSE: {sqrt(test_loss):.6f}')
            print(f'Validation MAE: {valid_loss_mae:.6f}, Test MAE: {test_loss_mae:.6f}')

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= 10:
                if True:
                    print(f"Early stopping at epoch {epoch+1}")
                break 

        model.train()

    if return_model:
        return model
    else:
        return valid_loss

study_name = "lm_hpo"
storage = "sqlite:///lm.db"
try: 
    study = optuna.create_study(
        study_name=study_name, 
        direction="minimize", 
        storage=storage, 
        load_if_exists=True,  
        pruner=optuna.pruners.SuccessiveHalvingPruner())
except:
    study = optuna.load_study(study_name=study_name, storage=storage)

study.optimize(
    run, 
    n_jobs=num_jobs,
    # n_trials=num_trials,
    callbacks=[MaxTrialsCallback(num_trials, states=(TrialState.COMPLETE,TrialState.PRUNED))]
)



# if htune:=True:
#     results = tuner.fit()
#     best_config = results.get_best_result().config
#     print(best_config)
#     best_config["print"] = True
#     model = run(best_config, return_model=True)
# else:
#     intial_config["print"] = True
#     model = run(intial_config, True)


# torch.save(model.state_dict(), '../models/lms/fine_tuned_qm9/good_model.pth')


# # Now, lets plot a scatter plot for the test dataset and the $R^2$ value.


# import matplotlib.pyplot as plt
# from sklearn.metrics import r2_score
# from scipy.stats import spearmanr


# model.eval()  # Set model to eval mode
# outputs_list = []

# y_true = normalized_targets[test_indices]
# with torch.no_grad():
#     model.eval()  # Set model to eval mode
#     test_loss = 0.0
#     test_loss_mae = 0.0
#     for batch_idx, (inputs, _targets) in enumerate(tqdm(test_dataloader)): 
#         inputs, _targets = inputs.to(device), _targets.to(device)
#         outputs = model(inputs)
#         # print(outputs[0], outputs[1])
#         # Calculate loss
#         loss = mse_loss(outputs.squeeze(1), _targets)
#         loss_mae = l1_loss(outputs.squeeze(1), _targets)
#         test_loss += loss.item() * inputs.size(0)
#         test_loss_mae += loss_mae.item() * inputs.size(0) 

#     test_loss /= len(test_dataloader.dataset)
#     test_loss_mae /= len(test_dataloader.dataset)

#     print(f'Test Loss (MSE): {test_loss:.6f}')
#     print(f'Test RMSE: {sqrt(test_loss):.6f}')
#     print(f'Test MAE: {test_loss_mae:.6f}')


#     y_pred = model(torch.stack(molecule_cls.embeddings)[test_indices].to(device)).to(torch.device("cpu")).cpu().numpy()
# # print(y_pred.size())

# r2 = r2_score(y_true.cpu().numpy(), y_pred)
# src, _ = spearmanr(y_true.cpu().numpy(), y_pred)

# # Create the plot
# plt.figure(figsize=(20, 20)) 
# plt.scatter(y_true, y_pred, alpha=0.7)  # Plot the data points

# # Add labels and title
# plt.xlabel("Actual Dipole Moment (Debye)", fontsize=12)
# plt.ylabel("Predicted Dipole Moment (Debye)", fontsize=12)
# plt.title("Dipole Moment Prediction - Language Model (CLS Token)", fontsize=14)

# # Add R-squared value to the plot
# plt.text(0.05, 0.9, f"$R^2$ = {r2:.3f}", fontsize=12, transform=plt.gca().transAxes)
# plt.text(0.05, 0.85, f"$r_s$ = {src:.3f}", fontsize=12, transform=plt.gca().transAxes)

# # Add a diagonal line for reference (perfect prediction)
# plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], linestyle='--', color='red')

# plt.grid(True) 
# plt.show()



# %load_ext tensorboard


